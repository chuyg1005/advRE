{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a70670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TACREDDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33aa4935",
   "metadata": {},
   "outputs": [],
   "source": [
    " from transformers import RobertaTokenizer, AutoTokenizerkenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7506f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 8 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True, use_fast=True)\n",
    "special_tokens_dict = {\"additional_special_tokens\":[\"<E>\", \"</E>\", \"<SUBJ>\", \"</SUBJ>\", \"<OBJ>\", \"</OBJ>\", \"<T>\", \"</T>\"]}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(\"We have added\", num_added_toks, \"tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f33776c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|███████████████████| 68124/68124 [00:01<00:00, 41036.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<OBJ>', 'Tom', 'Thabane', '</OBJ>', 'resigned', 'in', 'October', 'last', 'year', 'to', 'form', 'the', '<SUBJ>', 'All', 'Basotho', 'Convention', '</SUBJ>', '(', 'ABC', ')', ',', 'crossing', 'the', 'floor', 'with', '17', 'members', 'of', 'parliament', ',', 'causing', 'constitutional', 'monarch', 'King', 'Letsie', 'III', 'to', 'dissolve', 'parliament', 'and', 'call', 'the', 'snap', 'election', '.', '</s>', '</s>', 'Describe', 'the', 'relationship', 'between', 'organization', 'All', 'Basotho', 'Convention', 'and', 'person', 'Tom', 'Thabane', '.']\n",
      "organization founded by\n",
      "person religion\n",
      "['<SUBJ>', 'All', 'Basotho', 'Convention', '</SUBJ>']\n",
      "['<OBJ>', 'Tom', 'Thabane', '</OBJ>']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = TACREDDataset('data/tacred/train.json', use_pseudo=False)\n",
    "sent, pos, neg, ss, se, os, oe = dataset[0]\n",
    "print(sent)\n",
    "print(pos)\n",
    "print(neg)\n",
    "print(sent[ss:se+1])\n",
    "print(sent[os:oe+1])\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "300a161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "821dd4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_encodings', '_n_sequences', 'char_to_token', 'char_to_word', 'clear', 'convert_to_tensors', 'copy', 'data', 'encodings', 'fromkeys', 'get', 'is_fast', 'items', 'keys', 'n_sequences', 'pop', 'popitem', 'sequence_ids', 'setdefault', 'to', 'token_to_chars', 'token_to_sequence', 'token_to_word', 'tokens', 'update', 'values', 'word_ids', 'word_to_chars', 'word_to_tokens', 'words']\n",
      "{'input_ids': tensor([[    0, 50269,  1560,  2032,   873,  1728, 50270,  6490,    11,   779,\n",
      "            94,    76,     7,  1026,     5, 50267,   404,  7093,  6157,   139,\n",
      "          9127, 50268,    36,  3943,  4839,  2156,  6724,     5,  1929,    19,\n",
      "           601,   453,     9,  3589,  2156,  3735,  6100, 20303,  1745, 40702,\n",
      "           324,  6395,     7, 30887,  3589,     8,   486,     5,  6788,   729,\n",
      "           479,  1437,     2,  1437,     2, 27705, 21700,     5,  1291,   227,\n",
      "          1651,   404,  7093,  6157,   139,  9127,     8,   621,  1560,  2032,\n",
      "           873,  1728,   479,     2],\n",
      "        [    0,    96, 13668,  2156,    10,    76,    71,     5,  2669,  2156,\n",
      "         50267, 18095,  2865, 50268,   829,     5,    98,    12,  4155, 45518,\n",
      "         16333,  2354, 12801,    31,     5, 50269,   610,   211,     4, 50270,\n",
      "             8, 10530,   255,     4, 31357,  2475,   479,  1437,     2,  1437,\n",
      "             2, 27705, 21700,     5,  1291,   227,   621, 18095,  2865,     8,\n",
      "           621,   610,   211,     4,   479,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "sent_inputs = tokenizer([sent,dataset[1][0]], padding=True, truncation=True, max_length=128, return_tensors=\"pt\", is_split_into_words=True)\n",
    "print(type(sent_inputs))\n",
    "print(dir(sent_inputs))\n",
    "print(sent_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f6a6a1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "word_to_tokens() is not available when using Python based tokenizers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# help(sent_inputs.word_to_tokens)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m new_ss, _ \u001b[38;5;241m=\u001b[39m \u001b[43msent_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m _, new_se \u001b[38;5;241m=\u001b[39m sent_inputs\u001b[38;5;241m.\u001b[39mword_to_tokens(\u001b[38;5;241m0\u001b[39m, se)\n\u001b[1;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(sent_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][new_ss:new_se])\n",
      "File \u001b[0;32m/home/data_ti6_d/anaconda3/envs/chuyg-39/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:479\u001b[0m, in \u001b[0;36mBatchEncoding.word_to_tokens\u001b[0;34m(self, batch_or_word_index, word_index, sequence_index)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03mGet the encoded token span corresponding to a word in a sequence of the batch.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m    no tokens correspond to the word.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_to_tokens() is not available when using Python based tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     batch_index \u001b[38;5;241m=\u001b[39m batch_or_word_index\n",
      "\u001b[0;31mValueError\u001b[0m: word_to_tokens() is not available when using Python based tokenizers"
     ]
    }
   ],
   "source": [
    "# help(sent_inputs.word_to_tokens)\n",
    "new_ss, _ = sent_inputs.word_to_tokens(0, ss)\n",
    "_, new_se = sent_inputs.word_to_tokens(0, se)\n",
    "\n",
    "tokenizer.decode(sent_inputs['input_ids'][0][new_ss:new_se])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c20f497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True),\n",
       " 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True),\n",
       " 'additional_special_tokens': ['<E>',\n",
       "  '</E>',\n",
       "  '<SUBJ>',\n",
       "  '</SUBJ>',\n",
       "  '<OBJ>',\n",
       "  '</OBJ>',\n",
       "  '<T>',\n",
       "  '</T>']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa4405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
